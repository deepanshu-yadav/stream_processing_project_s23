{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1815f118-c7c1-423a-8eef-4e34244e091b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Schema for processing and storing streams\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, IntegerType\n",
    "\n",
    "EVENTS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"first_name\", StringType(), False),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True),\n",
    "        StructField(\"listen_timestamp\", LongType(), True),\n",
    "        StructField(\"song_id\", StringType(), True),\n",
    "        StructField(\"artist_name\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"song_title\", StringType(), True),\n",
    "        StructField(\"album_name\", StringType(), True),\n",
    "        StructField(\"release_date\", StringType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"danceability\", DoubleType(), True),\n",
    "        StructField(\"energy\", DoubleType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", DoubleType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", DoubleType(), True),\n",
    "        StructField(\"acousticness\", DoubleType(), True),\n",
    "        StructField(\"instrumentalness\", DoubleType(), True),\n",
    "        StructField(\"liveness\", DoubleType(), True),\n",
    "        StructField(\"valence\", DoubleType(), True),\n",
    "        StructField(\"tempo\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "PROCESSED_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"first_name\", StringType(), True),\n",
    "        StructField(\"last_name\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True),\n",
    "        StructField(\"listen_timestamp\", LongType(), True),\n",
    "        StructField(\"song_id\", StringType(), True),\n",
    "        StructField(\"artist_name\", StringType(), True),\n",
    "        StructField(\"artist_id\", StringType(), True),\n",
    "        StructField(\"song_title\", StringType(), True),\n",
    "        StructField(\"album_name\", StringType(), True),\n",
    "        StructField(\"release_date\", StringType(), True),\n",
    "        StructField(\"duration_ms\", LongType(), True),\n",
    "        StructField(\"danceability\", DoubleType(), True),\n",
    "        StructField(\"energy\", DoubleType(), True),\n",
    "        StructField(\"key\", IntegerType(), True),\n",
    "        StructField(\"loudness\", DoubleType(), True),\n",
    "        StructField(\"mode\", IntegerType(), True),\n",
    "        StructField(\"speechiness\", DoubleType(), True),\n",
    "        StructField(\"acousticness\", DoubleType(), True),\n",
    "        StructField(\"instrumentalness\", DoubleType(), True),\n",
    "        StructField(\"liveness\", DoubleType(), True),\n",
    "        StructField(\"valence\", DoubleType(), True),\n",
    "        StructField(\"tempo\", DoubleType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"month\", IntegerType(), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"day\", IntegerType(), True),\n",
    "        StructField(\"duration_minutes\", DoubleType(), True),\n",
    "        StructField(\"full_name\", StringType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1aaa487-542f-4e89-8915-27427cab50fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.179.0.4:9092\nspotify\nSpark Session Created\nRead Stream Created\nStream is Processed\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://denisecase.github.io/starting-spark/\n",
    "\"\"\"Script for processing kafka streams finding which song is most popular among indian males.\n",
    "\"\"\"\n",
    "import os\n",
    "from typing import List, Optional\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour\n",
    "from pyspark.sql.streaming import DataStreamReader\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "\n",
    "# load env variables\n",
    "# https://github.com/delta-io/delta/issues/593#issuecomment-816678840\n",
    "os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n",
    "\n",
    "# ===================================================================================\n",
    "#       LOAD ENVIRONMENT VARIABLES & SET CONFIGURATIONS\n",
    "# ===================================================================================\n",
    "KAFKA_TOPIC_NAME = \"spotify\"\n",
    "KAFKA_BOOTSTRAP_SERVER = \"10.179.0.4:9092\" # \"localhost:9092\"\n",
    "\n",
    "print(KAFKA_BOOTSTRAP_SERVER)\n",
    "print(KAFKA_TOPIC_NAME)\n",
    "\n",
    "# Required Spark  packages\n",
    "PACKAGES = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_or_get_spark(\n",
    "    app_name: str, packages: List[str], cluster_manager=\"local[*]\"\n",
    ") -> SparkSession:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        app_name (str): Name of the spark application\n",
    "        jars (str): List of jars needs to be installed before running spark application\n",
    "        cluster_manager (str, optional): cluster manager Defaults to \"local[*]\".\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: returns spark session\n",
    "    \"\"\"\n",
    "    jars = \",\".join(packages)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "        .config(\"spark.jars.packages\", jars)\n",
    "        .master(cluster_manager)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "def create_read_stream(\n",
    "    spark: SparkSession, broker_address: str, topic: str, offset: str = \"earliest\"\n",
    ") -> DataStreamReader:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): spark session\n",
    "        broker_address (str): kafka broker address Ex: localhost:9092\n",
    "        topic (str): topic from which events needs to consumed\n",
    "        offset (str, optional): _description_. Defaults to \"earliest\".\n",
    "\n",
    "    Returns:\n",
    "        DataStreamReader: Interface used to load a streaming DataFrame from external storage systems\n",
    "\n",
    "    Reference:\n",
    "        https://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html\n",
    "    \"\"\"\n",
    "    stream = (\n",
    "        spark.readStream.format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", broker_address)\n",
    "        .option(\"subscribe\", topic)\n",
    "        .option(\"startingOffsets\", offset)\n",
    "        .option(\n",
    "            \"key.deserializer\",\n",
    "            \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "        )\n",
    "        .option(\n",
    "            \"value.deserializer\",\n",
    "            \"org.apache.kafka.common.serialization.StringDeserializer\",\n",
    "        )\n",
    "        .option(\"failOnDataLoss\", False)\n",
    "        .load()\n",
    "    )\n",
    "    return stream\n",
    "\n",
    "\n",
    "def process_stream(df: DataFrame, schema: StructType) -> DataFrame:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): _description_\n",
    "        schema (StructType): _description_\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: _description_\n",
    "    \"\"\"\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df = df.select(F.from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "    # Add month, day, hour to split the data into separate directories\n",
    "    df = (\n",
    "        df.withColumn(\"ts\", F.to_timestamp(col(\"listen_timestamp\")))\n",
    "        .withColumn(\"year\", year(col(\"ts\")))\n",
    "        .withColumn(\"month\", month(col(\"ts\")))\n",
    "        .withColumn(\"hour\", hour(col(\"ts\")))\n",
    "        .withColumn(\"day\", dayofmonth(col(\"ts\")))\n",
    "        .drop(\"ts\")\n",
    "    )\n",
    "\n",
    "    # Add month, day, hour to split the data into separate directories\n",
    "    df = (\n",
    "        df.withColumn(\"duration_minutes\", F.round(col(\"duration_ms\") / 60000.0, 2))\n",
    "        .withColumn(\"latitude\", F.round(col(\"latitude\"), 3))\n",
    "        .withColumn(\"longitude\", F.round(col(\"longitude\"), 3))\n",
    "        .withColumn(\n",
    "            \"full_name\", F.concat(col(\"first_name\"), F.lit(\" \"), col(\"last_name\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_write_stream(\n",
    "    df: DataFrame, checkpoint_path: str, output_path: str, trigger: str = \"2 minutes\"\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): _description_\n",
    "        checkpoint_path (str): _description_\n",
    "        output_path (str): _description_\n",
    "        trigger (str, optional): _description_. Defaults to \"2 minutes\".\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    stream = (\n",
    "        df.writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .partitionBy(\"month\", \"day\", \"hour\")\n",
    "        .option(\"path\", output_path)\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .trigger(processingTime=trigger)\n",
    "    )\n",
    "    return stream\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                           MAIN ENTRYPOINT\n",
    "# ===================================================================================\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = create_or_get_spark(\n",
    "    app_name=\"spotify_streaming\", packages=PACKAGES, cluster_manager=\"local[*]\"\n",
    ")\n",
    "\n",
    "print(\"Spark Session Created\")\n",
    "\n",
    "# read stream events\n",
    "stream_df = create_read_stream(spark, KAFKA_BOOTSTRAP_SERVER, KAFKA_TOPIC_NAME)\n",
    "print(\"Read Stream Created\")\n",
    "\n",
    "# process stream events\n",
    "stream_df = process_stream(stream_df, EVENTS_SCHEMA)\n",
    "print(\"Stream is Processed\")\n",
    "\n",
    "# Filter data for Indian Males \n",
    "male_stream = stream_df.filter(stream_df.gender == \"male\") # & stream_df.country == \"India\")\n",
    "\n",
    "indian_male_stream = male_stream.filter(male_stream.country == \"India\")\n",
    "\n",
    "# Group data by song ID and count occurrences\n",
    "song_popularity_stream = indian_male_stream.groupBy(\"song_title\").count()\n",
    "\n",
    "# Find the most popular song\n",
    "most_popular_song_stream = song_popularity_stream.orderBy(col(\"count\").desc()).limit(1)\n",
    "\n",
    "# # Print the results\n",
    "query = most_popular_song_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81244fd-a12b-4171-a4c5-8ec56f640d00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark streaming notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
